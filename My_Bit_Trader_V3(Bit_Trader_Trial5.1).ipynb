{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modified-prison",
   "metadata": {},
   "source": [
    "## <프로젝트 유형: Multi-Step LSTM Time Series Forecasting>\n",
    "* windows개념 이용해서 univariate time series forecasting의 방법으로 해결해보고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-coordinate",
   "metadata": {},
   "source": [
    "# Version3\n",
    "### differed with Trial 5 by only batch_size(120 -> 128)\n",
    "## Results: (selling standard, quantity of selected samples, final outcome)\\ (1.05, 30, 17424), (1.08, 10, 18126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broke-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runned_by_Colab\n",
    "\n",
    "# Trial 2,5와 비교해서, window_size = 120이 60보다 나을까? possibly yes(윈도우 사이즈와 배치사이즈 소폭 조정(120->128)만 있었는데 1000원 정도의 성능 향상이 있었음)\n",
    "\n",
    "# <Trial 내용(V2에서 변화된 내용)>\n",
    "# 아래에 LSTM layer 하나 더 추가  <- (LSTM- BiLSTM - LSTM)모델로 Bi를 중간에 껴서 하니까 12269, 14831로 결과가 더 안 좋았다. \n",
    "# input(window size): 120, output: 1의 many-to-one 모델\n",
    "# reduceLROnplateau(factor= 0.9, patience = 5)\n",
    "# earlystopping(patience = 10)\n",
    "# batch_size 120 -> 128: 배치 사이즈로 보편적으로 활용되는 2의 거듭제곱 단위로 바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-entrepreneur",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Dropout, Bidirectional#,Reshape, Flatten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breeding-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "data_path = '/content/drive/MyDrive/Dacon_Data/[DACON]Bit_Trader/data' #'../Big_Data/[DACON]Bit_Trader'\n",
    "train_x_df = pd.read_csv(data_path  + \"/train_x_df.csv\")\n",
    "train_y_df = pd.read_csv(data_path  + \"/train_y_df.csv\")\n",
    "test_x_df = pd.read_csv(data_path  + \"/test_x_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desirable-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 받은 2차원 데이터 프레임을 3차원 numpy array로 변경하는 함수\n",
    "def df2d_to_array3d(df_2d):\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
    "    return array_3d\n",
    "\n",
    "# 2차원 DF LSTM으로 학습하기 위해 3차원으로 변환시키기\n",
    "train_x_array = df2d_to_array3d(train_x_df)   #(1380, 1380, 10)\n",
    "train_y_array = df2d_to_array3d(train_y_df)   #(1380, 120, 10)\n",
    "test_x_array = df2d_to_array3d(test_x_df)     #(529, 1380, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "talented-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120  #이제 window 사이즈 바꿀 때 얘만 변경해 주면 됨\n",
    "\n",
    "def build_model():\n",
    "    seq_len = 60\n",
    "    model = Sequential()\n",
    "    # 첫 번째 layer에서 LSTM()매서드 안의 stateful = True는 오히려 다운 퍼포먼스 될 수 있음\n",
    "    model.add(LSTM(100, activation='tanh', return_sequences= True, input_shape = [seq_len, 1])) \n",
    "#     model.add(Bidirectional(LSTM(50, activation='tanh', dropout=0.50, recurrent_dropout=0.25)))\n",
    "    model.add(LSTM(50, activation='tanh', return_sequences= True))\n",
    "    model.add(LSTM(50, activation= 'tanh'))\n",
    "    model.add(Dense(1))  #60개의 open값 입력되어 121번째 open값 '하나' 예측   \n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "korean-macro",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 60, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 50)            30200     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 91,251\n",
      "Trainable params: 91,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-cookie",
   "metadata": {
    "id": "Z6ImSUeoU2dO"
   },
   "outputs": [],
   "source": [
    "# LearningRateScheduler(scheduler) 구현\n",
    "\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# # LearningRateScheduler 함수 생성 (epoch이 5이하면 그대로 lr 유지, 아닐시 lr 조정)\n",
    "# def scheduler(epoch, learning_rate):\n",
    "#     if epoch < 5:\n",
    "#         return learning_rate\n",
    "#     else:\n",
    "#         return learning_rate * tf.math.exp(-0.1)\n",
    "# lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-recipient",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_x_array에 대한 Auto_Regressive한 Prediction 및 valid_pred_array에 예측 결과 기록\n",
    "\n",
    "# 1) test_pred_array{예측값 모아두는 3차원 배열(120*1 2차원 배열 529개)} 만들기\n",
    "test_pred_array = np.zeros([len(test_x_array), 120, 1])\n",
    "\n",
    "# 2) early_stoppage & reduceLR 정의: https://www.dacon.io/competitions/official/235709/codeshare/2453?page=1&dtype=recent 참고)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 10, mode = 'auto')\n",
    "reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.9, patience= 5, mode = 'auto', mindelta = 0.0001, min_lr=0)\n",
    "\n",
    "# 3) test_x_array로 시계열 Windows 만들기 -> 데이터셋 구성 -> 모델 학습 ||| 예측 -> test_pred_array에 기록 -> window_3d의 첫번째 값 삭제 -> test_pred_array와 window_3d 병합 -> model.predict()에 넣어 예측 -> ***\n",
    "ep = 30\n",
    "bs = 128\n",
    "# idx에 해당하는 샘플 529개 학습: for loop 529번 돌아감\n",
    "for idx in tqdm(range(test_x_array.shape[0])):  # 529번\n",
    "    sequence_length = seq_len + 1\n",
    "\n",
    "    windows = []\n",
    "    for index in range(1380 - sequence_length):\n",
    "        windows.append(test_x_array[idx, :, 1][index: index + sequence_length])\n",
    "\n",
    "    # x_test, y_test 데이터셋 구성\n",
    "    windows = np.array(windows)  # 1329 * 121의 2차원 배열\n",
    "    x_test = windows[:, :-1]\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    y_test = windows[:, -1]\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(x_test, y_test, epochs= ep, batch_size= bs, verbose=0, shuffle = True, callbacks=[early_stop, reduceLR])\n",
    "# shuffle= True 효과 있을까? 미약하게나마 효과 있음. https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "    print('sample_id : ', idx, '번')\n",
    "    print('loss : ', history.history['loss'][-1])\n",
    "#     print('mse : ', history.history['mse'][-1])  #<- model.compile()의 metrics인자값으로 설정된 값을 추가로 출력해 주고 싶다면!\n",
    "    print('lr : ', round(model.optimizer.lr.numpy(), 5))  #<-조정된 학습률 출력\n",
    "\n",
    "    # test_x_array Windows 중 마지막 윈도우 추출해서 3차원 변환시켜 LSTM모델에 넣고 Predict\n",
    "    window = windows[-1, :-1]  # windows.shape (1380-sequence_length, sequence_length), window.shape (seq_len, )\n",
    "    window_3d = np.reshape(window, (1, window.shape[0], 1))  # (1, seq_len, 1)\n",
    "    for m in range(120):\n",
    "        # model.predict()에 window_3d 넣어 예측\n",
    "        pred = model.predict(window_3d)\n",
    "\n",
    "        # 120분 중 처음 1분 예측값 test_pred_array에 기록\n",
    "        test_pred_array[idx, m, :] = pred\n",
    "\n",
    "        # window_3d의 첫번째 분 값은 삭제한 window_3d_2nd 구성\n",
    "        window_3d_2nd = window_3d[0, 1:, :]  # 119개\n",
    "\n",
    "        # pred_target(prediction할 때마다 나오는 각각의 예측값들) 1차원 -> 2차원으로 구성\n",
    "        pred_target = test_pred_array[idx, m, :]\n",
    "        pred_target = np.reshape(pred_target, (pred_target.shape[0], 1))\n",
    "\n",
    "        # test_pred_array와 window_3d_2nd 병합하여 모델에 입력할 새로운 window_3d 재구성\n",
    "        window_3d = np.concatenate((window_3d_2nd, pred_target), axis=0)\n",
    "        window_3d = window_3d.T\n",
    "        window_3d = np.reshape(window_3d, (window_3d.shape[0], window_3d.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x_array 각 샘플의 2시간 예측값이 기록되어 있는 test_pred_array shape 확인\n",
    "print(test_pred_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매수 시점, 매수 비율 표 만들기\n",
    "# 1) train_pred_array 3차원에서 2차원으로 바꾸기\n",
    "pred_array_2d = np.zeros([test_pred_array.shape[0], 120])\n",
    "\n",
    "for idx in tqdm(range(test_pred_array.shape[0])):\n",
    "    pred_array_2d[idx, :] = test_pred_array[idx, :, 0]\n",
    "\n",
    "# 2) 예측값을 재해석하여 submission 표를 작성하는 함수 정의\n",
    "def array_to_submission(pred_array):\n",
    "    submission = pd.DataFrame(np.zeros([pred_array.shape[0], 2], np.int64),\n",
    "                              columns=['buy_quantity', 'sell_time'])\n",
    "    submission = submission.reset_index()\n",
    "    sell_price = []\n",
    "    for idx, sell_time in enumerate(np.argmax(pred_array, axis=1)):\n",
    "        sell_price.append(pred_array[idx, sell_time])\n",
    "    sell_price = np.array(sell_price)\n",
    "    submission.loc[:, 'buy_quantity'] = ((1*1*(sell_price/1)*0.9995*0.9995) > 1.08)*1 #-DACON-Bit_Trader폴더 주가 손실계산.png 참고\n",
    "    submission['sell_time'] = np.argmax(pred_array, axis=1)\n",
    "    submission.columns = ['sample_id', 'buy_quantity', 'sell_time']\n",
    "    return submission, sell_price\n",
    "\n",
    "final_submission, forecasted_max = array_to_submission(pred_array_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_submission csv파일로 저장\n",
    "final_submission.to_csv('/content/drive/MyDrive/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 샘플의 예측치 중 최고값 모아 보기\n",
    "forecasted_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 300가지 sample에 대해 _가지 case에서 (수수료 감안해서) 매수 시점(1380분)보다 108% 이상 상승한다고 예측함.\n",
    "final_submission.buy_quantity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (수수료 감안하지 않고) 매수 시점보다 108% 이상 상승한다고 예측한 경우, 해당 예측치들 모아 보기\n",
    "forecasted_max[forecasted_max >= 1.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 및 로드\n",
    "model.save('./my_model.h5')\n",
    "model = tf.keras.models.load_model('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-immune",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 손실값 시각화\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-hello",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-state",
   "metadata": {},
   "source": [
    "매수할 샘플이 너무 적게 나왔을 경우 매수할 샘플이 10개 내외로 나오도록 매수 기준치 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "forcasted_max ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-passport",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "certain-examination",
    "outputId": "6e576de6-d83a-4dd8-ee22-0756b4a6d716"
   },
   "outputs": [],
   "source": [
    "len(forcasted_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-buffer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "capable-demographic",
    "outputId": "ce236eee-8419-4a81-e77c-b6cfdc5f62af"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in forcasted_max[forcasted_max>=1.10]:\n",
    "    l.append(i)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-interstate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "indoor-repository",
    "outputId": "d1e2c3fa-b95f-4a44-f0b1-65b6c2641e2f"
   },
   "outputs": [],
   "source": [
    "for j in l:\n",
    "    print(np.where(forcasted_max == j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-static",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가: test_x데이터로 예측하는 방식을 입력값(train_x)에 대한 예측값과 실제값(train_y_array) 비교를 통해 평가\n",
    "\n",
    "train_pred_array = np.zeros([1, 120, 1])\n",
    "\n",
    "sample = 100  #평가용 임의의 샘플 id\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 10, mode = 'auto')\n",
    "reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience= 5, mode = 'auto', mindelta = 0.0001, min_lr=0)\n",
    "\n",
    "ep = 30\n",
    "bs = 128\n",
    "# train_x_array데이터로 시계열 Windows 만들기\n",
    "# idx에 해당하는 샘플 하나만 학습하므로 for loop는 한 번만 돌아감\n",
    "for idx in range(sample, sample+1):\n",
    "    sequence_length = seq_len + 1\n",
    "\n",
    "    windows = []\n",
    "    for index in range(1380 - sequence_length):\n",
    "        windows.append(train_x_array[idx, :, 1][index: index + sequence_length])\n",
    "\n",
    "    # x_train, y_train 데이터 구성\n",
    "    windows = np.array(windows) #1329 * 121의 2차원 배열\n",
    "    x_train = windows[:, :-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    y_train = windows[:, -1]\n",
    "\n",
    "    \n",
    "    # Fit(early_stoppage & reduceLR 적용)\n",
    "    model = build_model()\n",
    "    history = model.fit(x_train, y_train, validation_split=0.1, epochs = ep, batch_size = bs, verbose = 2, callbacks = [early_stop, reduceLR])\n",
    "    print('sample_id : ', idx, '번')\n",
    "    print('loss : ', history.history['loss'][-1])\n",
    "#     print('mse : ', history.history['mse'][-1])  #<- model.compile()의 metrics인자값으로 설정된 값을 추가로 출력해 주고 싶다면!\n",
    "    print('lr : ', round(model.optimizer.lr.numpy(), 5))  #<-조정된 학습률 출력\n",
    "\n",
    "    # train_x_array Windows 중 마지막 윈도우 추출해서 3차원 변환시켜 LSTM모델에 넣고 Predict\n",
    "    window = windows[-1, :-1]\n",
    "    window_3d = np.reshape(window, (1, window.shape[0], 1))\n",
    "    for m in range(120):\n",
    "        # model.predict()에 window_3d 넣어 예측\n",
    "        pred = model.predict(window_3d)\n",
    "\n",
    "        # 120분 중 처음 1분 예측값 train_pred_array에 기록\n",
    "        train_pred_array[:, m, :] = pred\n",
    "\n",
    "        # window_3d의 첫번째 분 값은 삭제한 window_3d_2nd 구성\n",
    "        window_3d_2nd = window_3d[0, 1:, :]  # 119개\n",
    "\n",
    "        # pred_target(prediction할 때마다 나오는 각각의 예측값들) 1차원 -> 2차원으로 구성\n",
    "        pred_target = train_pred_array[:, m, :]\n",
    "        pred_target = np.reshape(pred_target, (pred_target.shape[0], 1))\n",
    "\n",
    "        # train_pred_array와 window_3d_2nd 병합하여 모델에 입력할 새로운 window_3d 재구성\n",
    "        # 이렇게 predict매서드에 들어갈 수 있는 형태로 만들어 줌으로써 얻은 이점: 1. Loop 돌리는 것 가능, 2. window가 이동하여 test_x의 마지막 윈도우의 값이 더 이상 남아 있지 않아도, 예상값들로만 새롭게 window를 구성하여 입력하는 방식으로 구현할 수 있게 해줌. \n",
    "        window_3d = np.concatenate((window_3d_2nd, pred_target), axis=0)\n",
    "        window_3d = window_3d.T\n",
    "        window_3d = np.reshape(window_3d, (window_3d.shape[0], window_3d.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 샘플 훈련 성과 시각화해보기\n",
    "# 1) 입력 series와 출력 series를 연속적으로 연결하여 시각적으로 보여주는 코드 정의\n",
    "def plot_series(x_series, y_series):\n",
    "    plt.plot(x_series, label = 'input_series')\n",
    "    plt.plot(np.arange(len(x_series), len(x_series)+len(y_series)),\n",
    "             y_series, label = 'output_series')\n",
    "    plt.axhline(1, c = 'red')\n",
    "    plt.legend()\n",
    "\n",
    "# 2) train data 중 sample_id idx에 해당하는 x_series로 모델을 학습한 후 y_series를 추론\n",
    "x_series = train_x_array[sample,:,1]\n",
    "y_series = train_y_array[sample,:,1]\n",
    "plot_series(x_series, y_series)\n",
    "plt.plot(np.arange(1380, 1380+120), train_pred_array[0,:,0], label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-curtis",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pred_array"
   ]
  },
  {
   "cell_type": "raw",
   "id": "authorized-machine",
   "metadata": {
    "id": "gMsdDfy93iRq"
   },
   "source": [
    "def df2d_to_answer(df_2d):\n",
    "    # valid_y_df로부터\n",
    "    # open 가격 정보가 포함된\n",
    "    # [샘플 수, 120분] 크기의 \n",
    "    # 2차원 array를 반환하는 함수\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "    sample_index = df_2d.sample_id.value_counts().index\n",
    "    array_2d = df_2d.open.values.reshape([sample_size, time_size])\n",
    "    sample_index = list(sample_index)\n",
    "    return array_2d, sample_index\n",
    "\n",
    "def COIN(y_df, submission, df2d_to_answer = df2d_to_answer):\n",
    "    # 2차원 데이터프레임에서 open 시점 데이터만 추출하여 array로 복원\n",
    "    # sample_id정보를 index에 저장\n",
    "    y_array, index = df2d_to_answer(y_df)\n",
    "    \n",
    "    # index 기준으로 submission을 다시 선택\n",
    "    submission = submission.set_index(submission.columns[0])\n",
    "    submission = submission.iloc[index, :]    \n",
    "    \n",
    "    # 초기 투자 비용은 10000 달러\n",
    "    total_momey      = 10000 # dolors\n",
    "    total_momey_list = []\n",
    "    \n",
    "    # 가장 처음 sample_id값\n",
    "    start_index = submission.index[0]\n",
    "    for row_idx in submission.index:\n",
    "        sell_time  = submission.loc[row_idx, 'sell_time']\n",
    "        buy_price  = y_array[row_idx - start_index, 0]\n",
    "        sell_price = y_array[row_idx - start_index, sell_time]\n",
    "        buy_quantity = submission.loc[row_idx, 'buy_quantity'] * total_momey\n",
    "        residual = total_momey - buy_quantity\n",
    "        ratio = sell_price / buy_price\n",
    "        total_momey = buy_quantity * ratio * 0.9995 * 0.9995 + residual        \n",
    "        total_momey_list.append(total_momey)\n",
    "        \n",
    "    return total_momey, total_momey_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실값 시각화\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wired-cartoon",
   "metadata": {
    "id": "yZkE1U804uGB"
   },
   "source": [
    "total_momey, total_momey_list = COIN(train_y_df,\n",
    "                                     train_pred_array)\n",
    "print(total_momey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-orlando",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "universal-detroit",
   "metadata": {},
   "source": [
    "# train_x_df의 자료들을 학습에 활용하게 될 경우:\n",
    "# train_x_df를 sample_id을 기준으로 추출하는 방법\n",
    "train_x_df = train_x_df[train_x_df.sample_id < 300]\n",
    "train_y_df = train_y_df[train_y_df.sample_id < 300]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
